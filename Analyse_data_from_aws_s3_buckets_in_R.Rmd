# Analyze data stored in AWS S3 using R 

Kiranmai Vasireddy(pv2342)

```{r}
library("aws.s3")
library(ggplot2)
library(dplyr)
library(tidyverse)
```

This post walks you through how data stored in an AWS account can be analyzed using R via the IDE: RStudio.

(Medium Post: https://medium.com/@kiranmaivp/analyse-data-stored-in-aws-s3-using-r-3cc73e17f563)

Introduction:

Over the recent years, we have seen a massive influx of data across all sectors of industries. Analyzing and Visualizing this data could help us learn about the underlying patterns in it. This data could be humongous to store on a local machine(PCs), making it impossible or potentially slowing down our local compute environment. And for this very reason, Cloud Service providers come to our rescue. It is significant in dealing with these challenges, it provides a platform to store a large amount of data, enhancing scalability and giving access to only a few users, thereby securing data. Storing data in the cloud also enables collaborating amongst various developers across regions working on a common problem using the same data.

One such service that empowers us to achieve all the above is the AWS S3. It allows us to store raw input files, results, reports, artifacts, and anything else, along with accessibility over the internet, making sharing resources with collaborators easy. In AWS S3, data is stored in buckets in the form of objects which can be accessed and manipulated based on the permissions given to the user. Also, as workload grows in R, migrating the files from the local compute environment to a fully managed cloud service, like AWS S3 provides a resilient system to handle data.

To analyze the data stored in an S3 bucket, we first have to access it. We use the R programming language as it is popular amongst statisticians, scientists, and data analysts for analyzing data. R has a large user community that developed thousands of freely available packages for data visualization, manipulation, statistical tests, Machine Learning, and Deep Learning. To access the data in the S3 bucket, we use one such package: aws.s3 to help us connect to AWS and its user buckets and work on the data from our local compute environment — RStudio in our local setup.

A quick review of a user and bucket in AWS and files stored in it is shown here. For demonstrating, I have created a free AWS account that provides access to all the AWS services- S3, EC2, IAM, etc.

1. User: The user “kiranmaivp” was created with “AmazonS3FullAccess” permissions to access all S3 buckets created in the AWS account. While creating the user, make sure to make a note of Access ID and Secret Access Key. These parameters are required to enable R to connect to the user from the RStudio IDE.

2. Bucket: An S3 bucket- kiranmai-edav-cc created has some files in it.

3. Files in a bucket: Each file in the bucket has an “S3 URI” parameter that allows access to the file. This value is unique for every file in that particular bucket.

Now that we have the necessary setup on the AWS side, we shall use the R package: aws.s3 to perform the following on the data from Rstudio IDE console.


1.  To connect to the AWS user from our IDE. We need to give three parameters to access the user- Access Key ID, Secret Access Key, and the AWS region where the setup is present.

```{r}

Sys.setenv(
      "AWS_ACCESS_KEY_ID" = "AKIASTX6MCRDBKNAL74M",
      "AWS_SECRET_ACCESS_KEY" = "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",
       "AWS_DEFAULT_REGION" = "us-east-2"
   )

```

2. After the connection is established, we can preview all the buckets the user can access:

```{r}
bucketlist()
```

3. We can also check the files present in the bucket using the bucket name:

```{r}
get_bucket(bucket = "kiranmai-edav-cc")
```

4. We can load the particular file in S3 by specifying its S3 URI in the s3read_using function call. This call loads the file to our IDE — Rstudio in a tabular format.

```{r}
data_fb <- s3read_using(FUN = data.table::fread, object = "s3://kiranmai-edav-cc/data.csv")
data_fb
```

5. Now we can perform various operations on the data. Here, I am just creating a simple scatter plot for the overall rating vs age of various players along with preferred foot for playing. It shows that more players prefer playing with their Right foot.

```{r}
ggplot(data_fb, aes(x=Age, y=Overall, color= data_fb$`Preferred Foot`)) + 
  geom_point()+xlab("Age") +
    ylab("Overall Rating")+
     ggtitle("Age vs Rating with Preferred Foot")
```

6.  We can plot many other plots and perform analysis on the data being read from Amazon s3 bucket. In this figure, I have plotted the top 15 nationality players along with their count.

```{r}
data <- data_fb %>% group_by(Nationality) %>% summarise(count=n()) %>% arrange(desc(count))

ggplot(data[1:15,], aes(x=fct_inorder(Nationality), y=count)) +
   geom_bar(fill = "lightblue",stat="identity")+
  xlab("Nationality")+
    ylab("Number of Players")+
  ggtitle("Top 15 Maximum number of Players from each Nation")

```

7. We can also write data into the S3 bucket using the put_object function, which creates the object named: Edav_cc.R in S3 bucket and stores the file “EDAV-cc.R” in it.

```{r}
put_object("EDAV-cc.R", object = "Edav_cc.R", bucket = "kiranmai-edav-cc")
```

8. Now, we can run the get_bucket function to check if the file upload is successful or check manually via AWS UI.

```{r}
get_bucket(bucket = "kiranmai-edav-cc")
```

An extension of this could be to read files from Amazon s3 to Amazon Textract and then process these files and tables in R, and then upload these tables into Amazon RDS.
The aws.s3 package offers numerous functions to allow access to data in S3 and R and work around it, which can be explored : https://cran.r-project.org/web/packages/aws.s3/aws.s3.pdf

Resources:
https://cran.r-project.org/web/packages/aws.s3/aws.s3.pdf
https://aws.amazon.com/blogs/opensource/getting-started-with-r-on-amazon-web-services/
https://aws.amazon.com/blogs/opensource/using-r-with-amazon-web-services-for-document-analysis/





